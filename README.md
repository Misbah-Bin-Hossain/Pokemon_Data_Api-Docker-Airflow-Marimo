{"canmore.create\_textdoc": {"name": "README", "type": "document", "content": "# 🎯 Pokémon ETL Pipeline with Airflow, Postgres, and Marimo\n\nWelcome to my **end-to-end ETL project** where I fetch data from the public [PokéAPI](https://pokeapi.co/), process it, and store it in a **Postgres database**.  \nI orchestrate the pipeline with **Apache Airflow**, visualize and explore with **Marimo notebooks**, and manage everything using **Docker Compose**.  \n\n---\n\n## 🛠️ Tools I Used & Why\n\n- **🐘 Postgres** → to persist Pokémon data (and keep it even if containers stop).  \n- **🌬️ Apache Airflow** → to orchestrate ETL tasks (Extract → Transform → Load) with scheduling and monitoring.  \n- **📦 Docker Compose** → to run multiple services (Airflow, Postgres, UV/Marimo) together in isolated containers.  \n- **🧩 Marimo (in UV container)** → interactive notebook environment to query Postgres, test ETL queries, and visualize results.  \n- **🐍 Python** → for the ETL scripts (using `requests`, `psycopg2`, `pandas`).\n\n---\n\n## 📂 Project Structure\n\n`bash\n.\n├── airflow/                  # Airflow setup\n│   ├── Dockerfile\n│   ├── dags/                 # DAGs live here\n│   │   └── etl_pipeline.py\n│   └── requirements.txt\n├── postgres/                 # Postgres setup\n│   └── init.sql              # Schema creation\n├── uv/                       # Marimo / Notebook container\n│   ├── Dockerfile\n│   ├── requirements.txt\n│   └── (optional app.py if API trigger is needed)\n├── docker-compose.yml         # Orchestrates all services\n├── pyproject.toml             # Python project dependencies\n└── README.md\n`\n\n---\n\n## 🚀 How to Run\n\n### 1. Clone this repo\n`bash\ngit clone https://github.com/<your-username>/etl-pipeline-project.git\ncd etl-pipeline-project\n`\n\n### 2. Start services with Docker Compose\n`bash\ndocker compose up --build\n`\n\nThis will start:  \n- `postgres` → database  \n- `airflow-webserver` → Airflow UI at [http://localhost:8080](http://localhost:8080)  \n- `airflow-scheduler` → runs DAGs  \n- `uv` → Marimo notebooks at [http://localhost:5000](http://localhost:5000)  \n\n### 3. Initialize Airflow\nSince Airflow needs DB setup and a user:\n`bash\ndocker compose run airflow-init\n`\n\nThen restart Airflow:\n`bash\ndocker compose restart airflow-webserver airflow-scheduler\n`\n\n✅ Login with:\n- username: `admin`\n- password: `admin`\n\n### 4. Access Marimo\nGo to [http://localhost:5000](http://localhost:5000) to open Marimo and query Postgres interactively.\n\n---\n\n## 📊 Database Schema\n\nTables created in Postgres (`init.sql`):\n\n`sql\nCREATE TABLE IF NOT EXISTS pokemon (\n    id INT PRIMARY KEY,\n    name TEXT,\n    height INT,\n    weight INT,\n    base_experience INT,\n    types TEXT[],\n    abilities TEXT[],\n    moves TEXT[],\n    stats JSONB\n);\n\nCREATE TABLE IF NOT EXISTS items (\n    id INT PRIMARY KEY,\n    name TEXT,\n    cost INT,\n    category TEXT,\n    effect TEXT\n);\n\nCREATE TABLE IF NOT EXISTS moves (\n    id INT PRIMARY KEY,\n    name TEXT,\n    power INT,\n    pp INT,\n    accuracy INT,\n    type TEXT,\n    damage_class TEXT\n);\n\nCREATE TABLE IF NOT EXISTS generations (\n    id INT PRIMARY KEY,\n    name TEXT,\n    main_region TEXT,\n    pokemon_species TEXT[],\n    moves TEXT[]\n);\n`\n\n---\n\n## 🔄 The ETL Flow\n\n1. **Extract** → fetch data from PokéAPI  \n2. **Transform** → clean/reshape into structured dicts  \n3. **Load** → insert into Postgres (using `psycopg2`)  \n\nIn Airflow (`etl_pipeline.py`):\n`python\nextract_task >> transform_task >> load_task\n`\n\nIn code (`main.py`):\n`python\npokemon_data = fetch_pokemon(limit=50)\nload_to_postgres(pokemon_data)\n`\n\n---\n\n## 🧪 Exploring with Marimo\n\nInside Marimo notebooks:\n`python\nimport pandas as pd\nimport psycopg2\n\nconn = psycopg2.connect(\"dbname=pokemon_db user=user password=password host=postgres port=5432\")\n\ndf = pd.read_sql(\"SELECT * FROM pokemon LIMIT 10;\", conn)\ndf.head()\n`\n\n✅ This lets you run **SQL directly against Postgres** and visualize results interactively.\n\n---\n\n## ⚡ Problems I Faced & Solutions\n\n### 1. ❌ Airflow DAGs not showing up\n- **Cause:** I put `airflow db init && airflow users create` inside `docker-compose` commands. Airflow services were racing to start before DB was ready.  \n- **Fix:** Moved DB init + user creation into a **separate `airflow-init` service**, ran it once before starting Airflow.\n\n---\n\n### 2. ❌ Arrays failing to insert in Postgres\n`error\npsycopg2.errors.InvalidTextRepresentation: malformed array literal\n`\n- **Cause:** I was trying to insert `['grass','poison']` as "grass,poison" instead of a Postgres array.  \n- **Fix:** Passed Python lists directly (`types TEXT[]`) and serialized dicts with `json.dumps`.\n\n---\n\n### 3. ❌ Marimo notebooks lost after container stop\n- **Cause:** No **volume** mounted in `uv` service.  \n- **Fix:** Added a volume so notebooks persist:\n`yaml\nuv:\n  volumes:\n    - ./notebooks:/app/notebooks\n`\n\n---\n\n### 4. ❌ Dependency conflicts (Uvicorn vs Marimo)\n- **Cause:** `uvicorn==0.15.0` in requirements conflicted with Marimo needing `uvicorn>=0.22.0`.  \n- **Fix:** Removed the pinned old version and let pip resolve the latest.\n\n---\n\n### 5. ❌ Airflow DAG duplicated ETL code\n- **Cause:** Had `main.py` both inside `python/etl` and `airflow/dags`.  \n- **Fix:** Mounted the ETL folder into Airflow, imported functions instead of duplicating code.\n\n---\n\n## 🧭 Lessons Learned\n\n- Separate **ETL logic** (in Python) from **orchestration** (Airflow).  \n- Always use **volumes** if you want persistence (Postgres data, notebooks, dags).  \n- Docker Compose makes multi-container workflows much easier.  \n- Airflow is powerful but requires careful initialization (DB + user).  \n- Marimo is a great lightweight notebook alternative for quick SQL + data exploration.\n\n---\n\n## ✅ Next Steps\n\n- Add CI/CD for running ETL tests before deployment.  \n- Expand database schema (abilities, trainers, etc.).  \n- Use Airflow connections instead of hardcoded credentials.  \n- Add visualization dashboards.\n\n---\n\n## 🖥️ Demo\n\n- **Airflow UI:** [http://localhost:8080](http://localhost:8080)  \n- **Marimo Notebook:** [http://localhost:5000](http://localhost:5000)  \n\n---\n\n## 🤝 Contributing\n\nPull requests welcome! If you’d like to add new DAGs, expand schema, or improve docs, feel free.\n\n---\n\n## 📜 License\nMIT License.\n"}}
