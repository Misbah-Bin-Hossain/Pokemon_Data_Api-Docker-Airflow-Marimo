# ğŸ¯ PokÃ©mon ETL Pipeline with Airflow, Postgres, and Marimo

Welcome to my **end-to-end ETL project** where I fetch data from the public [PokÃ©API](https://pokeapi.co/), process it, and store it in a **Postgres database**.  
I orchestrate the pipeline with **Apache Airflow**, visualize and explore with **Marimo notebooks**, and manage everything using **Docker Compose**.  

---

## ğŸ› ï¸ Tools I Used & Why

- **ğŸ˜ Postgres** â†’ to persist PokÃ©mon data (and keep it even if containers stop).  
- **ğŸŒ¬ï¸ Apache Airflow** â†’ to orchestrate ETL tasks (Extract â†’ Transform â†’ Load) with scheduling and monitoring.  
- **ğŸ“¦ Docker Compose** â†’ to run multiple services (Airflow, Postgres, UV/Marimo) together in isolated containers.  
- **ğŸ§© Marimo (in UV container)** â†’ interactive notebook environment to query Postgres, test ETL queries, and visualize results.  
- **ğŸ Python** â†’ for the ETL scripts (using `requests`, `psycopg2`, `pandas`).

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ airflow/                  # Airflow setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ dags/                 # DAGs live here
â”‚   â”‚   â””â”€â”€ etl_pipeline.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ postgres/                 # Postgres setup
â”‚   â””â”€â”€ init.sql              # Schema creation
â”œâ”€â”€ uv/                       # Marimo / Notebook container
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ (optional app.py if API trigger is needed)
â”œâ”€â”€ docker-compose.yml         # Orchestrates all services
â”œâ”€â”€ pyproject.toml             # Python project dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

### 1. Clone this repo
```bash
git clone https://github.com/<your-username>/etl-pipeline-project.git
cd etl-pipeline-project
```

### 2. Start services with Docker Compose
```bash
docker compose up --build
```

This will start:  
- `postgres` â†’ database  
- `airflow-webserver` â†’ Airflow UI at [http://localhost:8080](http://localhost:8080)  
- `airflow-scheduler` â†’ runs DAGs  
- `uv` â†’ Marimo notebooks at [http://localhost:5000](http://localhost:5000)  

### 3. Initialize Airflow
Since Airflow needs DB setup and a user:
```bash
docker compose run airflow-init
```

Then restart Airflow:
```bash
docker compose restart airflow-webserver airflow-scheduler
```

âœ… Login with:
- username: `admin`
- password: `admin`

### 4. Access Marimo
Go to [http://localhost:5000](http://localhost:5000) to open Marimo and query Postgres interactively.

---

## ğŸ“Š Database Schema

Tables created in Postgres (`init.sql`):

```sql
CREATE TABLE IF NOT EXISTS pokemon (
    id INT PRIMARY KEY,
    name TEXT,
    height INT,
    weight INT,
    base_experience INT,
    types TEXT[],
    abilities TEXT[],
    moves TEXT[],
    stats JSONB
);

CREATE TABLE IF NOT EXISTS items (
    id INT PRIMARY KEY,
    name TEXT,
    cost INT,
    category TEXT,
    effect TEXT
);

CREATE TABLE IF NOT EXISTS moves (
    id INT PRIMARY KEY,
    name TEXT,
    power INT,
    pp INT,
    accuracy INT,
    type TEXT,
    damage_class TEXT
);

CREATE TABLE IF NOT EXISTS generations (
    id INT PRIMARY KEY,
    name TEXT,
    main_region TEXT,
    pokemon_species TEXT[],
    moves TEXT[]
);
```

---

## ğŸ”„ The ETL Flow

1. **Extract** â†’ fetch data from PokÃ©API  
2. **Transform** â†’ clean/reshape into structured dicts  
3. **Load** â†’ insert into Postgres (using `psycopg2`)  

In Airflow (`etl_pipeline.py`):
```python
extract_task >> transform_task >> load_task
```

In code (`main.py`):
```python
pokemon_data = fetch_pokemon(limit=50)
load_to_postgres(pokemon_data)
```

---

## ğŸ§ª Exploring with Marimo

Inside Marimo notebooks:
```python
import pandas as pd
import psycopg2

conn = psycopg2.connect("dbname=pokemon_db user=user password=password host=postgres port=5432")

df = pd.read_sql("SELECT * FROM pokemon LIMIT 10;", conn)
df.head()
```

âœ… This lets you run **SQL directly against Postgres** and visualize results interactively.

---

## âš¡ Problems I Faced & Solutions

### 1. âŒ Airflow DAGs not showing up
- **Cause:** I put `airflow db init && airflow users create` inside `docker-compose` commands. Airflow services were racing to start before DB was ready.  
- **Fix:** Moved DB init + user creation into a **separate `airflow-init` service**, ran it once before starting Airflow.

---

### 2. âŒ Arrays failing to insert in Postgres
```error
psycopg2.errors.InvalidTextRepresentation: malformed array literal
```
- **Cause:** I was trying to insert `['grass','poison']` as "grass,poison" instead of a Postgres array.  
- **Fix:** Passed Python lists directly (`types TEXT[]`) and serialized dicts with `json.dumps`.

---

### 3. âŒ Marimo notebooks lost after container stop
- **Cause:** No **volume** mounted in `uv` service.  
- **Fix:** Added a volume so notebooks persist:
```yaml
uv:
  volumes:
    - ./notebooks:/app/notebooks
```

---

### 4. âŒ Dependency conflicts (Uvicorn vs Marimo)
- **Cause:** `uvicorn==0.15.0` in requirements conflicted with Marimo needing `uvicorn>=0.22.0`.  
- **Fix:** Removed the pinned old version and let pip resolve the latest.

---

### 5. âŒ Airflow DAG duplicated ETL code
- **Cause:** Had `main.py` both inside `python/etl` and `airflow/dags`.  
- **Fix:** Mounted the ETL folder into Airflow, imported functions instead of duplicating code.

---

## ğŸ§­ Lessons Learned

- Separate **ETL logic** (in Python) from **orchestration** (Airflow).  
- Always use **volumes** if you want persistence (Postgres data, notebooks, dags).  
- Docker Compose makes multi-container workflows much easier.  
- Airflow is powerful but requires careful initialization (DB + user).  
- Marimo is a great lightweight notebook alternative for quick SQL + data exploration.

---

## âœ… Next Steps

- Add CI/CD for running ETL tests before deployment.  
- Expand database schema (abilities, trainers, etc.).  
- Use Airflow connections instead of hardcoded credentials.  
- Add visualization dashboards.

---

## ğŸ–¥ï¸ Demo

- **Airflow UI:** [http://localhost:8080](http://localhost:8080)  
- **Marimo Notebook:** [http://localhost:5000](http://localhost:5000)  
<img width="888" height="991" alt="image" src="https://github.com/user-attachments/assets/57a2ccc9-c5d1-48a9-9a87-4cd9a60afc0b" />

---
image.png

## ğŸ¤ Contributing

Pull requests welcome! If youâ€™d like to add new DAGs, expand schema, or improve docs, feel free.

---

## ğŸ“œ License
MIT License.
