{"canmore.create\_textdoc": {"name": "README", "type": "document", "content": "# ğŸ¯ PokÃ©mon ETL Pipeline with Airflow, Postgres, and Marimo\n\nWelcome to my **end-to-end ETL project** where I fetch data from the public [PokÃ©API](https://pokeapi.co/), process it, and store it in a **Postgres database**.  \nI orchestrate the pipeline with **Apache Airflow**, visualize and explore with **Marimo notebooks**, and manage everything using **Docker Compose**.  \n\n---\n\n## ğŸ› ï¸ Tools I Used & Why\n\n- **ğŸ˜ Postgres** â†’ to persist PokÃ©mon data (and keep it even if containers stop).  \n- **ğŸŒ¬ï¸ Apache Airflow** â†’ to orchestrate ETL tasks (Extract â†’ Transform â†’ Load) with scheduling and monitoring.  \n- **ğŸ“¦ Docker Compose** â†’ to run multiple services (Airflow, Postgres, UV/Marimo) together in isolated containers.  \n- **ğŸ§© Marimo (in UV container)** â†’ interactive notebook environment to query Postgres, test ETL queries, and visualize results.  \n- **ğŸ Python** â†’ for the ETL scripts (using `requests`, `psycopg2`, `pandas`).\n\n---\n\n## ğŸ“‚ Project Structure\n\n`bash\n.\nâ”œâ”€â”€ airflow/                  # Airflow setup\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ dags/                 # DAGs live here\nâ”‚   â”‚   â””â”€â”€ etl_pipeline.py\nâ”‚   â””â”€â”€ requirements.txt\nâ”œâ”€â”€ postgres/                 # Postgres setup\nâ”‚   â””â”€â”€ init.sql              # Schema creation\nâ”œâ”€â”€ uv/                       # Marimo / Notebook container\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ (optional app.py if API trigger is needed)\nâ”œâ”€â”€ docker-compose.yml         # Orchestrates all services\nâ”œâ”€â”€ pyproject.toml             # Python project dependencies\nâ””â”€â”€ README.md\n`\n\n---\n\n## ğŸš€ How to Run\n\n### 1. Clone this repo\n`bash\ngit clone https://github.com/<your-username>/etl-pipeline-project.git\ncd etl-pipeline-project\n`\n\n### 2. Start services with Docker Compose\n`bash\ndocker compose up --build\n`\n\nThis will start:  \n- `postgres` â†’ database  \n- `airflow-webserver` â†’ Airflow UI at [http://localhost:8080](http://localhost:8080)  \n- `airflow-scheduler` â†’ runs DAGs  \n- `uv` â†’ Marimo notebooks at [http://localhost:5000](http://localhost:5000)  \n\n### 3. Initialize Airflow\nSince Airflow needs DB setup and a user:\n`bash\ndocker compose run airflow-init\n`\n\nThen restart Airflow:\n`bash\ndocker compose restart airflow-webserver airflow-scheduler\n`\n\nâœ… Login with:\n- username: `admin`\n- password: `admin`\n\n### 4. Access Marimo\nGo to [http://localhost:5000](http://localhost:5000) to open Marimo and query Postgres interactively.\n\n---\n\n## ğŸ“Š Database Schema\n\nTables created in Postgres (`init.sql`):\n\n`sql\nCREATE TABLE IF NOT EXISTS pokemon (\n    id INT PRIMARY KEY,\n    name TEXT,\n    height INT,\n    weight INT,\n    base_experience INT,\n    types TEXT[],\n    abilities TEXT[],\n    moves TEXT[],\n    stats JSONB\n);\n\nCREATE TABLE IF NOT EXISTS items (\n    id INT PRIMARY KEY,\n    name TEXT,\n    cost INT,\n    category TEXT,\n    effect TEXT\n);\n\nCREATE TABLE IF NOT EXISTS moves (\n    id INT PRIMARY KEY,\n    name TEXT,\n    power INT,\n    pp INT,\n    accuracy INT,\n    type TEXT,\n    damage_class TEXT\n);\n\nCREATE TABLE IF NOT EXISTS generations (\n    id INT PRIMARY KEY,\n    name TEXT,\n    main_region TEXT,\n    pokemon_species TEXT[],\n    moves TEXT[]\n);\n`\n\n---\n\n## ğŸ”„ The ETL Flow\n\n1. **Extract** â†’ fetch data from PokÃ©API  \n2. **Transform** â†’ clean/reshape into structured dicts  \n3. **Load** â†’ insert into Postgres (using `psycopg2`)  \n\nIn Airflow (`etl_pipeline.py`):\n`python\nextract_task >> transform_task >> load_task\n`\n\nIn code (`main.py`):\n`python\npokemon_data = fetch_pokemon(limit=50)\nload_to_postgres(pokemon_data)\n`\n\n---\n\n## ğŸ§ª Exploring with Marimo\n\nInside Marimo notebooks:\n`python\nimport pandas as pd\nimport psycopg2\n\nconn = psycopg2.connect(\"dbname=pokemon_db user=user password=password host=postgres port=5432\")\n\ndf = pd.read_sql(\"SELECT * FROM pokemon LIMIT 10;\", conn)\ndf.head()\n`\n\nâœ… This lets you run **SQL directly against Postgres** and visualize results interactively.\n\n---\n\n## âš¡ Problems I Faced & Solutions\n\n### 1. âŒ Airflow DAGs not showing up\n- **Cause:** I put `airflow db init && airflow users create` inside `docker-compose` commands. Airflow services were racing to start before DB was ready.  \n- **Fix:** Moved DB init + user creation into a **separate `airflow-init` service**, ran it once before starting Airflow.\n\n---\n\n### 2. âŒ Arrays failing to insert in Postgres\n`error\npsycopg2.errors.InvalidTextRepresentation: malformed array literal\n`\n- **Cause:** I was trying to insert `['grass','poison']` as "grass,poison" instead of a Postgres array.  \n- **Fix:** Passed Python lists directly (`types TEXT[]`) and serialized dicts with `json.dumps`.\n\n---\n\n### 3. âŒ Marimo notebooks lost after container stop\n- **Cause:** No **volume** mounted in `uv` service.  \n- **Fix:** Added a volume so notebooks persist:\n`yaml\nuv:\n  volumes:\n    - ./notebooks:/app/notebooks\n`\n\n---\n\n### 4. âŒ Dependency conflicts (Uvicorn vs Marimo)\n- **Cause:** `uvicorn==0.15.0` in requirements conflicted with Marimo needing `uvicorn>=0.22.0`.  \n- **Fix:** Removed the pinned old version and let pip resolve the latest.\n\n---\n\n### 5. âŒ Airflow DAG duplicated ETL code\n- **Cause:** Had `main.py` both inside `python/etl` and `airflow/dags`.  \n- **Fix:** Mounted the ETL folder into Airflow, imported functions instead of duplicating code.\n\n---\n\n## ğŸ§­ Lessons Learned\n\n- Separate **ETL logic** (in Python) from **orchestration** (Airflow).  \n- Always use **volumes** if you want persistence (Postgres data, notebooks, dags).  \n- Docker Compose makes multi-container workflows much easier.  \n- Airflow is powerful but requires careful initialization (DB + user).  \n- Marimo is a great lightweight notebook alternative for quick SQL + data exploration.\n\n---\n\n## âœ… Next Steps\n\n- Add CI/CD for running ETL tests before deployment.  \n- Expand database schema (abilities, trainers, etc.).  \n- Use Airflow connections instead of hardcoded credentials.  \n- Add visualization dashboards.\n\n---\n\n## ğŸ–¥ï¸ Demo\n\n- **Airflow UI:** [http://localhost:8080](http://localhost:8080)  \n- **Marimo Notebook:** [http://localhost:5000](http://localhost:5000)  \n\n---\n\n## ğŸ¤ Contributing\n\nPull requests welcome! If youâ€™d like to add new DAGs, expand schema, or improve docs, feel free.\n\n---\n\n## ğŸ“œ License\nMIT License.\n"}}
