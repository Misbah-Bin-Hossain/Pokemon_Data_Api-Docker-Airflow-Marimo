# üéØ Pok√©mon ETL Pipeline with Airflow, Postgres, and Marimo

###Welcome to my **end-to-end ETL project** 
                    ![pokemon-drake](https://github.com/user-attachments/assets/2f94c9e1-4397-4162-a5cd-3127ccae71f5)


Here I fetch data from the public [Pok√©API](https://pokeapi.co/), process it, and store it in a **Postgres database**.  
I orchestrate the pipeline with **Apache Airflow**, visualize and explore with **Marimo notebooks**, and manage everything using **Docker Compose**.  

---

## üõ†Ô∏è Tools I Used & Why

- **üêò Postgres** ‚Üí to persist Pok√©mon data (and keep it even if containers stop).  
- **üå¨Ô∏è Apache Airflow** ‚Üí to orchestrate ETL tasks (Extract ‚Üí Transform ‚Üí Load) with scheduling and monitoring.  
- **üì¶ Docker Compose** ‚Üí to run multiple services (Airflow, Postgres, Python/Marimo) together in isolated containers.  
- **üß© Marimo (in python_etl container)** ‚Üí interactive notebook environment to query Postgres, test ETL queries, and visualize results.  
- **üêç Python** ‚Üí for the ETL scripts (using `requests`, `psycopg2`, `pandas`).

---



## üìÇ Project Structure

```bash
.
‚îú‚îÄ‚îÄ airflow/                  # Airflow setup
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ dags/                 # DAGs live here
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ postgres/                 # Postgres setup
‚îÇ   ‚îî‚îÄ‚îÄ init.sql              # Schema creation
‚îú‚îÄ‚îÄ docker-compose.yml         # Orchestrates all services
‚îú‚îÄ‚îÄ pyproject.toml             # Python project dependencies
‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ python/                    # Python ETL scripts & Marimo notebooks
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ etl.py
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/             # Notebooks live here
```

---

## üöÄ How to Run

### 1. Clone this repo
```bash
git clone https://github.com/Misbah-Bin-Hossain/Pokemon_Data_Api-Docker-Airflow-Marimo.git
cd Pokemon_Data_Api-Docker-Airflow-Marimo
```

### 2. Start services with Docker Compose
```bash
docker compose up --build
```

This will start:  
- `postgres` ‚Üí database  
- `airflow-webserver` ‚Üí Airflow UI at [http://localhost:8080](http://localhost:8080)  
- `airflow-scheduler` ‚Üí runs DAGs  
- `python_etl` ‚Üí Marimo notebooks at [http://localhost:8888](http://localhost:8888)  

### 3. Login Airflow

`airflow-webserver` ‚Üí Airflow UI at [http://localhost:8080](http://localhost:8080)  


‚úÖ Login with:
- username: `admin`
- password: `admin`

### 4. Access Marimo

Marimo starts automatically in the `python_etl` container. Just open your browser and go to:

[http://localhost:8888](http://localhost:8888)
Then enter the Access token provided in the Terminal

No need to manually enter the container, create a virtual environment, or install packages‚Äîthis is all handled by the Dockerfile during build.

If you want to run a shell in the container for debugging or custom scripts:
```bash
docker exec -it python_etl bash
```
--Inside Marimo install the required packages to see the visuals (This is to make process faster, as if its is automated, it will take more time)
    
    "Then after Installation Shift to Toggle View to have a better Visual"


---

## üìä Database Schema

Tables created in Postgres (`init.sql`):

```sql
CREATE TABLE IF NOT EXISTS pokemon (
    id INT PRIMARY KEY,
    name TEXT,
    height INT,
    weight INT,
    base_experience INT,
    types TEXT[],
    abilities TEXT[],
    moves TEXT[],
    stats JSONB
);

CREATE TABLE IF NOT EXISTS items (
    id INT PRIMARY KEY,
    name TEXT,
    cost INT,
    category TEXT,
    effect TEXT
);

CREATE TABLE IF NOT EXISTS moves (
    id INT PRIMARY KEY,
    name TEXT,
    power INT,
    pp INT,
    accuracy INT,
    type TEXT,
    damage_class TEXT
);

CREATE TABLE IF NOT EXISTS generations (
    id INT PRIMARY KEY,
    name TEXT,
    main_region TEXT,
    pokemon_species TEXT[],
    moves TEXT[]
);
```

---

## üîÑ The ETL Flow

1. **Extract** ‚Üí fetch data from Pok√©API  
2. **Transform** ‚Üí clean/reshape into structured dicts  
3. **Load** ‚Üí insert into Postgres (using `psycopg2`)  

In Airflow (`etl_pipeline.py`):
```python
extract_task >> transform_task >> load_task
```

In code (`main.py`):
```python
pokemon_data = fetch_pokemon(limit=50)
load_to_postgres(pokemon_data)
```

---

## üß™ Exploring with Marimo

Inside Marimo notebooks:
```python
import pandas as pd
import psycopg2

conn = psycopg2.connect("dbname=pokemon_db user=user password=password host=postgres port=5432")

df = pd.read_sql("SELECT * FROM pokemon LIMIT 10;", conn)
df.head()
```

‚úÖ This lets you run **SQL directly against Postgres** and visualize results interactively.

---

## ‚ö° Problems I Faced & Solutions

### 1. ‚ùå Airflow DAGs not showing up
- **Cause:** I put `airflow db init && airflow users create` inside `docker-compose` commands. Airflow services were racing to start before DB was ready.  
- **Fix:** Moved DB init + user creation into a **separate `airflow-init` service**, ran it once before starting Airflow.

---

### 2. ‚ùå Arrays failing to insert in Postgres
```error
psycopg2.errors.InvalidTextRepresentation: malformed array literal
```
- **Cause:** I was trying to insert `['grass','poison']` as "grass,poison" instead of a Postgres array.  
- **Fix:** Passed Python lists directly (`types TEXT[]`) and serialized dicts with `json.dumps`.

---

### 3. ‚ùå Marimo notebooks lost after container stop
- **Cause:** No **volume** mounted in `python` service.  
- **Fix:** Added a volume so notebooks persist:
```yaml
python_etl:
  volumes:
    - ./notebooks:/app/notebooks
```

---

### 4. ‚ùå Dependency conflicts (Uvicorn vs Marimo)
- **Cause:** `uvicorn==0.15.0` in requirements conflicted with Marimo needing `uvicorn>=0.22.0`.  
- **Fix:** Removed the pinned old version and let pip resolve the latest.

---

### 5. ‚ùå Airflow DAG duplicated ETL code
- **Cause:** Had `main.py` both inside `python/etl` and `airflow/dags`.  
- **Fix:** Mounted the ETL folder into Airflow, imported functions instead of duplicating code.

---

## üß≠ Lessons Learned

- Separate **ETL logic** (in Python) from **orchestration** (Airflow).  
- Always use **volumes** if you want persistence (Postgres data, notebooks, dags).  
- Docker Compose makes multi-container workflows much easier.  
- Airflow is powerful but requires careful initialization (DB + user).  
- Marimo is a great lightweight notebook alternative for quick SQL + data exploration.

---

## ‚úÖ Next Steps

- Add CI/CD for running ETL tests before deployment.  
- Expand database schema (abilities, trainers, etc.).  
- Use Airflow connections instead of hardcoded credentials.  
- Add visualization dashboards.

---

## üñ•Ô∏è Demo

- **Airflow UI:** [http://localhost:8080](http://localhost:8080)  
- **Marimo Notebook:** [http://localhost:8888](http://localhost:8888)  
<img width="888" height="991" alt="image" src="https://github.com/user-attachments/assets/57a2ccc9-c5d1-48a9-9a87-4cd9a60afc0b" />

---
image.png

## ü§ù Contributing

Pull requests welcome! If you‚Äôd like to add new DAGs, expand schema, or improve docs, feel free.

---
